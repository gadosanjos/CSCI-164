{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the backend using tensorflow\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "# Suppress tensorflow INFO messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The backend must be set before importing keras, not after\n",
    "import keras\n",
    "keras.utils.set_random_seed(812)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  carat        cut color clarity  depth  table  price     x  \\\n",
      "0               0   0.23      Ideal     E     SI2   61.5   55.0    326  3.95   \n",
      "1               1   0.21    Premium     E     SI1   59.8   61.0    326  3.89   \n",
      "2               2   0.23       Good     E     VS1   56.9   65.0    327  4.05   \n",
      "3               3   0.29    Premium     I     VS2   62.4   58.0    334  4.20   \n",
      "4               4   0.31       Good     J     SI2   63.3   58.0    335  4.34   \n",
      "...           ...    ...        ...   ...     ...    ...    ...    ...   ...   \n",
      "53935       53935   0.72      Ideal     D     SI1   60.8   57.0   2757  5.75   \n",
      "53936       53936   0.72       Good     D     SI1   63.1   55.0   2757  5.69   \n",
      "53937       53937   0.70  Very Good     D     SI1   62.8   60.0   2757  5.66   \n",
      "53938       53938   0.86    Premium     H     SI2   61.0   58.0   2757  6.15   \n",
      "53939       53939   0.75      Ideal     D     SI2   62.2   55.0   2757  5.83   \n",
      "\n",
      "          y     z  \n",
      "0      3.98  2.43  \n",
      "1      3.84  2.31  \n",
      "2      4.07  2.31  \n",
      "3      4.23  2.63  \n",
      "4      4.35  2.75  \n",
      "...     ...   ...  \n",
      "53935  5.76  3.50  \n",
      "53936  5.75  3.61  \n",
      "53937  5.68  3.56  \n",
      "53938  6.12  3.74  \n",
      "53939  5.87  3.64  \n",
      "\n",
      "[53940 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "diamonds = pd.read_csv('Diamonds.csv')\n",
    "print (diamonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  carat        cut color clarity  depth  table  price     x  \\\n",
      "0               0   0.23      Ideal     E     SI2   61.5   55.0    326  3.95   \n",
      "1               1   0.21    Premium     E     SI1   59.8   61.0    326  3.89   \n",
      "2               2   0.23       Good     E     VS1   56.9   65.0    327  4.05   \n",
      "3               3   0.29    Premium     I     VS2   62.4   58.0    334  4.20   \n",
      "4               4   0.31       Good     J     SI2   63.3   58.0    335  4.34   \n",
      "...           ...    ...        ...   ...     ...    ...    ...    ...   ...   \n",
      "53935       53935   0.72      Ideal     D     SI1   60.8   57.0   2757  5.75   \n",
      "53936       53936   0.72       Good     D     SI1   63.1   55.0   2757  5.69   \n",
      "53937       53937   0.70  Very Good     D     SI1   62.8   60.0   2757  5.66   \n",
      "53938       53938   0.86    Premium     H     SI2   61.0   58.0   2757  6.15   \n",
      "53939       53939   0.75      Ideal     D     SI2   62.2   55.0   2757  5.83   \n",
      "\n",
      "          y     z  cutR  colorR  clarityR  \n",
      "0      3.98  2.43     5       6         2  \n",
      "1      3.84  2.31     4       6         3  \n",
      "2      4.07  2.31     2       6         5  \n",
      "3      4.23  2.63     4       2         4  \n",
      "4      4.35  2.75     2       1         2  \n",
      "...     ...   ...   ...     ...       ...  \n",
      "53935  5.76  3.50     5       7         3  \n",
      "53936  5.75  3.61     2       7         3  \n",
      "53937  5.68  3.56     3       7         3  \n",
      "53938  6.12  3.74     4       3         2  \n",
      "53939  5.87  3.64     5       7         2  \n",
      "\n",
      "[53940 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "def Cut (series): \n",
    "\n",
    "    if series == \"Ideal\":\n",
    "        return 5\n",
    "\n",
    "    if series == \"Premium\": \n",
    "        return 4\n",
    "\n",
    "    if series == \"Very Good\": \n",
    "        return 3\n",
    "\n",
    "    if series == \"Good\": \n",
    "        return 2\n",
    "    \n",
    "    if series == \"Fair\":\n",
    "        return 1\n",
    "    \n",
    "\n",
    "diamonds['cutR'] = diamonds['cut'].apply(Cut)\n",
    "\n",
    "\n",
    "# ### Recoding Color to ColorR\n",
    "\n",
    "# In[66]:\n",
    "\n",
    "\n",
    "def Color (series): \n",
    "\n",
    "    if series == \"D\":\n",
    "        return 7\n",
    "\n",
    "    if series == \"E\": \n",
    "        return 6\n",
    "\n",
    "    if series == \"F\": \n",
    "        return 5\n",
    "\n",
    "    if series == \"G\": \n",
    "        return 4\n",
    "    \n",
    "    if series == \"H\":\n",
    "        return 3\n",
    "    \n",
    "    if series == \"I\":\n",
    "        return 2\n",
    "    \n",
    "    if series == \"J\":\n",
    "        return 1\n",
    "    \n",
    "\n",
    "diamonds['colorR'] = diamonds['color'].apply(Color)\n",
    "\n",
    "\n",
    "# ### Recoding Clarity to ClarityR\n",
    "\n",
    "# In[67]:\n",
    "\n",
    "\n",
    "def Clarity (series): \n",
    "\n",
    "    if series == \"I1\":\n",
    "        return 1\n",
    "\n",
    "    if series == \"SI2\": \n",
    "        return 2\n",
    "\n",
    "    if series == \"SI1\": \n",
    "        return 3\n",
    "\n",
    "    if series == \"VS2\": \n",
    "        return 4\n",
    "    \n",
    "    if series == \"VS1\":\n",
    "        return 5\n",
    "    \n",
    "    if series == \"VVS2\":\n",
    "        return 6\n",
    "    \n",
    "    if series == \"VVS1\":\n",
    "        return 7\n",
    "    \n",
    "    if series == \"IF\":\n",
    "        return 8\n",
    "    \n",
    "\n",
    "diamonds['clarityR'] = diamonds['clarity'].apply(Clarity)\n",
    "\n",
    "\n",
    "# ### Dataset with new columns: cutR, colorR, clarityR \n",
    "\n",
    "# In[68]:\n",
    "\n",
    "\n",
    "print(diamonds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = diamonds[['carat', 'cutR', 'colorR', 'clarityR']]\n",
    "y = diamonds['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = .4, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32364, 4) (32364,)\n",
      "(21576, 4) (21576,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        # Input layer\n",
    "        keras.layers.Input(shape=(4, )),\n",
    "        # Hidden layer 1 = 256 nodes, linear activation\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        # Hidden layer 2: 128 nodes, linear activation\n",
    "        keras.layers.Dense(128, activation='linear'),\n",
    "         # Hidden layer 3: 64 nodes, linear activation\n",
    "        keras.layers.Dense(64, activation='linear'),\n",
    "        # Output layer: 1 node\n",
    "        keras.layers.Dense(1, activation='linear'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42,497</span> (166.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m42,497\u001b[0m (166.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42,497</span> (166.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m42,497\u001b[0m (166.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='Adam',  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss='MeanSquaredError',\n",
    "    # List of metrics to monitor\n",
    "    metrics=['mse'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 18931080.0000 - mse: 18931080.0000 - val_loss: 2652204.0000 - val_mse: 2652204.0000\n",
      "Epoch 2/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841us/step - loss: 2185940.7500 - mse: 2185940.7500 - val_loss: 1264799.3750 - val_mse: 1264799.3750\n",
      "Epoch 3/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - loss: 1151149.2500 - mse: 1151149.2500 - val_loss: 892037.8125 - val_mse: 892037.8125\n",
      "Epoch 4/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 868508.6875 - mse: 868508.6875 - val_loss: 721810.8125 - val_mse: 721810.8125\n",
      "Epoch 5/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 734117.4375 - mse: 734117.4375 - val_loss: 614185.1250 - val_mse: 614185.1250\n",
      "Epoch 6/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - loss: 644981.1250 - mse: 644981.1250 - val_loss: 546837.6250 - val_mse: 546837.6250\n",
      "Epoch 7/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - loss: 587093.8750 - mse: 587093.8750 - val_loss: 500732.5625 - val_mse: 500732.5625\n",
      "Epoch 8/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 549161.0000 - mse: 549161.0000 - val_loss: 468144.6250 - val_mse: 468144.6250\n",
      "Epoch 9/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 521970.3750 - mse: 521970.3750 - val_loss: 444760.2500 - val_mse: 444760.2500\n",
      "Epoch 10/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 503824.5938 - mse: 503824.5938 - val_loss: 427270.6875 - val_mse: 427270.6875\n",
      "Epoch 11/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 489087.5938 - mse: 489087.5938 - val_loss: 414743.2500 - val_mse: 414743.2500\n",
      "Epoch 12/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 476944.8750 - mse: 476944.8750 - val_loss: 399937.9688 - val_mse: 399937.9688\n",
      "Epoch 13/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 466501.0625 - mse: 466501.0625 - val_loss: 391569.4375 - val_mse: 391569.4375\n",
      "Epoch 14/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 458373.6562 - mse: 458373.6562 - val_loss: 385299.9375 - val_mse: 385299.9375\n",
      "Epoch 15/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - loss: 451813.5938 - mse: 451813.5938 - val_loss: 381171.0625 - val_mse: 381171.0625\n",
      "Epoch 16/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 445609.2188 - mse: 445609.2188 - val_loss: 377458.7188 - val_mse: 377458.7188\n",
      "Epoch 17/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 440798.1562 - mse: 440798.1562 - val_loss: 372065.9375 - val_mse: 372065.9375\n",
      "Epoch 18/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - loss: 437769.7188 - mse: 437769.7188 - val_loss: 369053.9375 - val_mse: 369053.9375\n",
      "Epoch 19/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 433935.7188 - mse: 433935.7188 - val_loss: 367658.2812 - val_mse: 367658.2812\n",
      "Epoch 20/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 430830.4375 - mse: 430830.4375 - val_loss: 364529.7500 - val_mse: 364529.7500\n",
      "Epoch 21/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 427694.6875 - mse: 427694.6875 - val_loss: 362305.4375 - val_mse: 362305.4375\n",
      "Epoch 22/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 424609.8438 - mse: 424609.8438 - val_loss: 360526.5625 - val_mse: 360526.5625\n",
      "Epoch 23/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - loss: 421834.2188 - mse: 421834.2188 - val_loss: 359386.3438 - val_mse: 359386.3438\n",
      "Epoch 24/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 418906.1250 - mse: 418906.1250 - val_loss: 355248.4062 - val_mse: 355248.4062\n",
      "Epoch 25/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 415995.1250 - mse: 415995.1250 - val_loss: 353270.5000 - val_mse: 353270.5000\n",
      "Epoch 26/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - loss: 413822.8125 - mse: 413822.8125 - val_loss: 351902.9688 - val_mse: 351902.9688\n",
      "Epoch 27/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 411089.5312 - mse: 411089.5312 - val_loss: 351188.6562 - val_mse: 351188.6562\n",
      "Epoch 28/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - loss: 408993.0625 - mse: 408993.0625 - val_loss: 348339.8438 - val_mse: 348339.8438\n",
      "Epoch 29/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 406521.6562 - mse: 406521.6562 - val_loss: 348111.4688 - val_mse: 348111.4688\n",
      "Epoch 30/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 404690.5625 - mse: 404690.5625 - val_loss: 347398.8750 - val_mse: 347398.8750\n",
      "Epoch 31/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - loss: 402918.1875 - mse: 402918.1875 - val_loss: 345152.3438 - val_mse: 345152.3438\n",
      "Epoch 32/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 401231.4688 - mse: 401231.4688 - val_loss: 344634.9688 - val_mse: 344634.9688\n",
      "Epoch 33/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - loss: 399957.1875 - mse: 399957.1875 - val_loss: 344004.6562 - val_mse: 344004.6562\n",
      "Epoch 34/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - loss: 398974.0312 - mse: 398974.0312 - val_loss: 345313.4375 - val_mse: 345313.4375\n",
      "Epoch 35/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - loss: 397916.4375 - mse: 397916.4375 - val_loss: 342714.8125 - val_mse: 342714.8125\n",
      "Epoch 36/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 395998.1875 - mse: 395998.1875 - val_loss: 343774.1250 - val_mse: 343774.1250\n",
      "Epoch 37/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - loss: 394721.9062 - mse: 394721.9062 - val_loss: 342647.1562 - val_mse: 342647.1562\n",
      "Epoch 38/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - loss: 393733.4688 - mse: 393733.4688 - val_loss: 341415.1875 - val_mse: 341415.1875\n",
      "Epoch 39/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832us/step - loss: 392580.1250 - mse: 392580.1250 - val_loss: 341375.1562 - val_mse: 341375.1562\n",
      "Epoch 40/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795us/step - loss: 391901.8750 - mse: 391901.8750 - val_loss: 340900.6562 - val_mse: 340900.6562\n",
      "Epoch 41/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 391299.5000 - mse: 391299.5000 - val_loss: 341133.0000 - val_mse: 341133.0000\n",
      "Epoch 42/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 391099.8750 - mse: 391099.8750 - val_loss: 340551.7188 - val_mse: 340551.7188\n",
      "Epoch 43/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795us/step - loss: 390043.3125 - mse: 390043.3125 - val_loss: 338669.9375 - val_mse: 338669.9375\n",
      "Epoch 44/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 389374.8438 - mse: 389374.8438 - val_loss: 339636.5938 - val_mse: 339636.5938\n",
      "Epoch 45/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 389314.2188 - mse: 389314.2188 - val_loss: 337667.3125 - val_mse: 337667.3125\n",
      "Epoch 46/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 388143.3438 - mse: 388143.3438 - val_loss: 337823.5000 - val_mse: 337823.5000\n",
      "Epoch 47/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - loss: 387711.9062 - mse: 387711.9062 - val_loss: 339050.9375 - val_mse: 339050.9375\n",
      "Epoch 48/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 387122.0938 - mse: 387122.0938 - val_loss: 337468.9375 - val_mse: 337468.9375\n",
      "Epoch 49/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 386190.3750 - mse: 386190.3750 - val_loss: 335949.0625 - val_mse: 335949.0625\n",
      "Epoch 50/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 385448.1250 - mse: 385448.1250 - val_loss: 336479.0312 - val_mse: 336479.0312\n",
      "Epoch 51/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - loss: 385200.5312 - mse: 385200.5312 - val_loss: 335581.6250 - val_mse: 335581.6250\n",
      "Epoch 52/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 384230.6875 - mse: 384230.6875 - val_loss: 334679.2188 - val_mse: 334679.2188\n",
      "Epoch 53/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 383825.7812 - mse: 383825.7812 - val_loss: 336764.1562 - val_mse: 336764.1562\n",
      "Epoch 54/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 383408.4688 - mse: 383408.4688 - val_loss: 337048.3125 - val_mse: 337048.3125\n",
      "Epoch 55/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 382469.5625 - mse: 382469.5625 - val_loss: 334403.6562 - val_mse: 334403.6562\n",
      "Epoch 56/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 381994.7188 - mse: 381994.7188 - val_loss: 335341.9375 - val_mse: 335341.9375\n",
      "Epoch 57/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 381349.6250 - mse: 381349.6250 - val_loss: 336563.2500 - val_mse: 336563.2500\n",
      "Epoch 58/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 380797.9375 - mse: 380797.9375 - val_loss: 334276.9375 - val_mse: 334276.9375\n",
      "Epoch 59/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 380261.5000 - mse: 380261.5000 - val_loss: 334832.6562 - val_mse: 334832.6562\n",
      "Epoch 60/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 834us/step - loss: 379825.2500 - mse: 379825.2500 - val_loss: 333655.5625 - val_mse: 333655.5625\n",
      "Epoch 61/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 378999.0938 - mse: 378999.0938 - val_loss: 334871.0938 - val_mse: 334871.0938\n",
      "Epoch 62/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 378709.5938 - mse: 378709.5938 - val_loss: 334061.6562 - val_mse: 334061.6562\n",
      "Epoch 63/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781us/step - loss: 377984.1875 - mse: 377984.1875 - val_loss: 332906.2188 - val_mse: 332906.1875\n",
      "Epoch 64/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 377589.7500 - mse: 377589.7500 - val_loss: 332382.5312 - val_mse: 332382.5312\n",
      "Epoch 65/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 785us/step - loss: 377321.7500 - mse: 377321.7500 - val_loss: 333848.4688 - val_mse: 333848.4688\n",
      "Epoch 66/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 376803.0000 - mse: 376803.0000 - val_loss: 332402.9062 - val_mse: 332402.9062\n",
      "Epoch 67/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 376278.1562 - mse: 376278.1562 - val_loss: 330172.4688 - val_mse: 330172.4688\n",
      "Epoch 68/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step - loss: 376311.0000 - mse: 376311.0000 - val_loss: 332911.5000 - val_mse: 332911.5000\n",
      "Epoch 69/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 376294.3438 - mse: 376294.3438 - val_loss: 331799.9375 - val_mse: 331799.9375\n",
      "Epoch 70/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795us/step - loss: 375381.4688 - mse: 375381.4688 - val_loss: 332880.4375 - val_mse: 332880.4375\n",
      "Epoch 71/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 374314.6562 - mse: 374314.6562 - val_loss: 332492.4688 - val_mse: 332492.4688\n",
      "Epoch 72/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835us/step - loss: 374274.2812 - mse: 374274.2812 - val_loss: 332862.0000 - val_mse: 332862.0000\n",
      "Epoch 73/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 373600.1250 - mse: 373600.1250 - val_loss: 332256.7188 - val_mse: 332256.7188\n",
      "Epoch 74/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 373407.4375 - mse: 373407.4375 - val_loss: 332075.6250 - val_mse: 332075.6250\n",
      "Epoch 75/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 373403.8750 - mse: 373403.8750 - val_loss: 332334.0938 - val_mse: 332334.0938\n",
      "Epoch 76/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 372760.7500 - mse: 372760.7500 - val_loss: 331015.4688 - val_mse: 331015.4688\n",
      "Epoch 77/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 372435.5938 - mse: 372435.5938 - val_loss: 332619.1250 - val_mse: 332619.1250\n",
      "Epoch 78/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 371815.6250 - mse: 371815.6250 - val_loss: 329591.6875 - val_mse: 329591.6875\n",
      "Epoch 79/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 371285.2500 - mse: 371285.2500 - val_loss: 330539.0312 - val_mse: 330539.0312\n",
      "Epoch 80/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 371094.8438 - mse: 371094.8438 - val_loss: 333470.3750 - val_mse: 333470.3750\n",
      "Epoch 81/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 370454.0938 - mse: 370454.0938 - val_loss: 334597.7812 - val_mse: 334597.7812\n",
      "Epoch 82/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 370324.0312 - mse: 370324.0312 - val_loss: 329679.7500 - val_mse: 329679.7500\n",
      "Epoch 83/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - loss: 370374.4375 - mse: 370374.4375 - val_loss: 332620.8438 - val_mse: 332620.8438\n",
      "Epoch 84/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 369808.6250 - mse: 369808.6250 - val_loss: 331205.7500 - val_mse: 331205.7500\n",
      "Epoch 85/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 369302.0000 - mse: 369302.0000 - val_loss: 331416.0938 - val_mse: 331416.0938\n",
      "Epoch 86/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 785us/step - loss: 369036.8750 - mse: 369036.8750 - val_loss: 330913.4375 - val_mse: 330913.4375\n",
      "Epoch 87/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 368261.0312 - mse: 368261.0312 - val_loss: 331726.4375 - val_mse: 331726.4375\n",
      "Epoch 88/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832us/step - loss: 368781.3438 - mse: 368781.3438 - val_loss: 332805.1250 - val_mse: 332805.1250\n",
      "Epoch 89/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 368381.1562 - mse: 368381.1562 - val_loss: 334205.6562 - val_mse: 334205.6562\n",
      "Epoch 90/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step - loss: 368163.9688 - mse: 368163.9688 - val_loss: 333239.4375 - val_mse: 333239.4375\n",
      "Epoch 91/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 367360.1562 - mse: 367360.1562 - val_loss: 331025.9062 - val_mse: 331025.9062\n",
      "Epoch 92/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 367355.2500 - mse: 367355.2500 - val_loss: 331728.0625 - val_mse: 331728.0625\n",
      "Epoch 93/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 366625.6562 - mse: 366625.6562 - val_loss: 330862.5938 - val_mse: 330862.5938\n",
      "Epoch 94/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795us/step - loss: 366664.9688 - mse: 366664.9688 - val_loss: 330304.3750 - val_mse: 330304.3750\n",
      "Epoch 95/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 366004.2812 - mse: 366004.2812 - val_loss: 333898.6562 - val_mse: 333898.6562\n",
      "Epoch 96/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 365790.2500 - mse: 365790.2500 - val_loss: 328223.9688 - val_mse: 328223.9688\n",
      "Epoch 97/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 366030.0312 - mse: 366030.0312 - val_loss: 327792.0000 - val_mse: 327792.0000\n",
      "Epoch 98/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795us/step - loss: 365364.9062 - mse: 365364.9062 - val_loss: 328826.6250 - val_mse: 328826.6250\n",
      "Epoch 99/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 365132.8125 - mse: 365132.8125 - val_loss: 329234.1875 - val_mse: 329234.1875\n",
      "Epoch 100/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 364327.4062 - mse: 364327.4062 - val_loss: 330451.4688 - val_mse: 330451.4688\n",
      "Epoch 101/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step - loss: 364987.3750 - mse: 364987.3750 - val_loss: 330767.4062 - val_mse: 330767.4062\n",
      "Epoch 102/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 363994.7812 - mse: 363994.7812 - val_loss: 328555.7812 - val_mse: 328555.7812\n",
      "Epoch 103/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - loss: 363420.0000 - mse: 363420.0000 - val_loss: 329422.1875 - val_mse: 329422.1875\n",
      "Epoch 104/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 363519.4062 - mse: 363519.4062 - val_loss: 327891.0938 - val_mse: 327891.0938\n",
      "Epoch 105/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795us/step - loss: 363216.5000 - mse: 363216.5000 - val_loss: 326862.2812 - val_mse: 326862.2812\n",
      "Epoch 106/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 362909.8750 - mse: 362909.8750 - val_loss: 329622.5625 - val_mse: 329622.5625\n",
      "Epoch 107/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 362550.2500 - mse: 362550.2500 - val_loss: 328685.5625 - val_mse: 328685.5625\n",
      "Epoch 108/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 362013.9688 - mse: 362013.9688 - val_loss: 329078.2500 - val_mse: 329078.2500\n",
      "Epoch 109/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 362179.2500 - mse: 362179.2500 - val_loss: 328265.2812 - val_mse: 328265.2812\n",
      "Epoch 110/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 362156.9688 - mse: 362156.9688 - val_loss: 326701.8125 - val_mse: 326701.8125\n",
      "Epoch 111/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - loss: 361488.3125 - mse: 361488.3125 - val_loss: 327010.7812 - val_mse: 327010.7812\n",
      "Epoch 112/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 361841.8438 - mse: 361841.8438 - val_loss: 327248.5625 - val_mse: 327248.5625\n",
      "Epoch 113/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 361751.5938 - mse: 361751.5938 - val_loss: 328966.9688 - val_mse: 328966.9688\n",
      "Epoch 114/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 362127.6250 - mse: 362127.6250 - val_loss: 328965.8438 - val_mse: 328965.8438\n",
      "Epoch 115/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 360566.2188 - mse: 360566.2188 - val_loss: 325269.2500 - val_mse: 325269.2500\n",
      "Epoch 116/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - loss: 360682.3750 - mse: 360682.3750 - val_loss: 327581.9062 - val_mse: 327581.9062\n",
      "Epoch 117/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 360327.7500 - mse: 360327.7500 - val_loss: 327117.9062 - val_mse: 327117.9062\n",
      "Epoch 118/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 360208.1562 - mse: 360208.1562 - val_loss: 325725.6250 - val_mse: 325725.6250\n",
      "Epoch 119/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 360565.6250 - mse: 360565.6250 - val_loss: 328109.4688 - val_mse: 328109.4688\n",
      "Epoch 120/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 360541.2500 - mse: 360541.2500 - val_loss: 325820.5312 - val_mse: 325820.5312\n",
      "Epoch 121/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 359605.0625 - mse: 359605.0625 - val_loss: 328095.1562 - val_mse: 328095.1562\n",
      "Epoch 122/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 359455.3438 - mse: 359455.3438 - val_loss: 328554.0938 - val_mse: 328554.0938\n",
      "Epoch 123/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 359760.3438 - mse: 359760.3438 - val_loss: 325229.4375 - val_mse: 325229.4375\n",
      "Epoch 124/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 359396.7812 - mse: 359396.7812 - val_loss: 326206.0000 - val_mse: 326206.0000\n",
      "Epoch 125/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832us/step - loss: 358703.8750 - mse: 358703.8750 - val_loss: 327357.0625 - val_mse: 327357.0625\n",
      "Epoch 126/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 358246.3125 - mse: 358246.3125 - val_loss: 327182.0312 - val_mse: 327182.0312\n",
      "Epoch 127/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - loss: 358778.9062 - mse: 358778.9062 - val_loss: 326498.4688 - val_mse: 326498.4688\n",
      "Epoch 128/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 359027.6562 - mse: 359027.6562 - val_loss: 326156.3125 - val_mse: 326156.3125\n",
      "Epoch 129/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 358757.1562 - mse: 358757.1562 - val_loss: 324293.0312 - val_mse: 324293.0312\n",
      "Epoch 130/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 357853.3438 - mse: 357853.3438 - val_loss: 325752.5312 - val_mse: 325752.5312\n",
      "Epoch 131/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 357572.4062 - mse: 357572.4062 - val_loss: 326379.4375 - val_mse: 326379.4375\n",
      "Epoch 132/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 357929.5000 - mse: 357929.5000 - val_loss: 324441.5938 - val_mse: 324441.5938\n",
      "Epoch 133/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784us/step - loss: 357757.9688 - mse: 357757.9688 - val_loss: 324531.9062 - val_mse: 324531.9062\n",
      "Epoch 134/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - loss: 357651.3438 - mse: 357651.3438 - val_loss: 324218.0938 - val_mse: 324218.0938\n",
      "Epoch 135/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 356890.9688 - mse: 356890.9688 - val_loss: 324422.5000 - val_mse: 324422.5000\n",
      "Epoch 136/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 356145.7500 - mse: 356145.7500 - val_loss: 322954.8125 - val_mse: 322954.8125\n",
      "Epoch 137/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 357147.5625 - mse: 357147.5625 - val_loss: 323335.9375 - val_mse: 323335.9375\n",
      "Epoch 138/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 356846.8125 - mse: 356846.8125 - val_loss: 323802.1875 - val_mse: 323802.1875\n",
      "Epoch 139/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - loss: 356692.4688 - mse: 356692.4688 - val_loss: 326711.7500 - val_mse: 326711.7500\n",
      "Epoch 140/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 356938.8125 - mse: 356938.8125 - val_loss: 325702.0625 - val_mse: 325702.0625\n",
      "Epoch 141/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 356103.0000 - mse: 356103.0000 - val_loss: 323733.5312 - val_mse: 323733.5312\n",
      "Epoch 142/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 355447.4688 - mse: 355447.4688 - val_loss: 321942.9062 - val_mse: 321942.9062\n",
      "Epoch 143/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 834us/step - loss: 356021.0312 - mse: 356021.0312 - val_loss: 325773.3125 - val_mse: 325773.3125\n",
      "Epoch 144/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 355443.6875 - mse: 355443.6875 - val_loss: 324361.5312 - val_mse: 324361.5312\n",
      "Epoch 145/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 355503.7500 - mse: 355503.7500 - val_loss: 323317.0000 - val_mse: 323317.0000\n",
      "Epoch 146/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - loss: 355340.1562 - mse: 355340.1562 - val_loss: 324933.8125 - val_mse: 324933.8125\n",
      "Epoch 147/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 355659.5312 - mse: 355659.5312 - val_loss: 323802.0312 - val_mse: 323802.0312\n",
      "Epoch 148/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 355241.5000 - mse: 355241.5000 - val_loss: 323995.8750 - val_mse: 323995.8750\n",
      "Epoch 149/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 355026.0625 - mse: 355026.0625 - val_loss: 324701.8438 - val_mse: 324701.8438\n",
      "Epoch 150/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 354626.4062 - mse: 354626.4062 - val_loss: 324961.8125 - val_mse: 324961.8125\n",
      "Epoch 151/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 354405.7500 - mse: 354405.7500 - val_loss: 324871.7500 - val_mse: 324871.7500\n",
      "Epoch 152/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 354445.3125 - mse: 354445.3125 - val_loss: 324282.2500 - val_mse: 324282.2500\n",
      "Epoch 153/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 354311.5938 - mse: 354311.5938 - val_loss: 323231.5000 - val_mse: 323231.5000\n",
      "Epoch 154/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 850us/step - loss: 354565.5938 - mse: 354565.5938 - val_loss: 326487.4062 - val_mse: 326487.4062\n",
      "Epoch 155/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 354488.3750 - mse: 354488.3750 - val_loss: 324359.1875 - val_mse: 324359.1875\n",
      "Epoch 156/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 353808.5938 - mse: 353808.5938 - val_loss: 322964.3750 - val_mse: 322964.3750\n",
      "Epoch 157/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - loss: 353992.4688 - mse: 353992.4688 - val_loss: 323829.3750 - val_mse: 323829.3750\n",
      "Epoch 158/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 353834.8750 - mse: 353834.8750 - val_loss: 325802.9688 - val_mse: 325802.9688\n",
      "Epoch 159/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 838us/step - loss: 353868.8125 - mse: 353868.8125 - val_loss: 323953.4062 - val_mse: 323953.4062\n",
      "Epoch 160/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 353805.8750 - mse: 353805.8750 - val_loss: 323010.9375 - val_mse: 323010.9375\n",
      "Epoch 161/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 353329.0000 - mse: 353329.0000 - val_loss: 324625.4062 - val_mse: 324625.4062\n",
      "Epoch 162/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 353494.6250 - mse: 353494.6250 - val_loss: 323464.7188 - val_mse: 323464.7188\n",
      "Epoch 163/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784us/step - loss: 353353.3125 - mse: 353353.3125 - val_loss: 324652.0625 - val_mse: 324652.0625\n",
      "Epoch 164/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 353138.7500 - mse: 353138.7500 - val_loss: 323942.6875 - val_mse: 323942.6875\n",
      "Epoch 165/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 352575.7812 - mse: 352575.7812 - val_loss: 323631.4062 - val_mse: 323631.4062\n",
      "Epoch 166/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 352723.8750 - mse: 352723.8750 - val_loss: 325120.6250 - val_mse: 325120.6250\n",
      "Epoch 167/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - loss: 352425.8125 - mse: 352425.8125 - val_loss: 323575.5938 - val_mse: 323575.5938\n",
      "Epoch 168/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 352493.9062 - mse: 352493.9062 - val_loss: 326364.1875 - val_mse: 326364.1875\n",
      "Epoch 169/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 352875.0000 - mse: 352875.0000 - val_loss: 322641.8125 - val_mse: 322641.8125\n",
      "Epoch 170/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 352157.5000 - mse: 352157.5000 - val_loss: 326226.4375 - val_mse: 326226.4375\n",
      "Epoch 171/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 352211.4375 - mse: 352211.4375 - val_loss: 323915.2812 - val_mse: 323915.2812\n",
      "Epoch 172/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step - loss: 352123.4062 - mse: 352123.4062 - val_loss: 327734.6875 - val_mse: 327734.6875\n",
      "Epoch 173/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - loss: 352433.8125 - mse: 352433.8125 - val_loss: 325946.1250 - val_mse: 325946.1250\n",
      "Epoch 174/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 838us/step - loss: 351645.8438 - mse: 351645.8438 - val_loss: 327453.4062 - val_mse: 327453.4062\n",
      "Epoch 175/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 351668.1562 - mse: 351668.1562 - val_loss: 326459.3750 - val_mse: 326459.3750\n",
      "Epoch 176/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 351277.7188 - mse: 351277.7188 - val_loss: 323621.7500 - val_mse: 323621.7500\n",
      "Epoch 177/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step - loss: 351239.6250 - mse: 351239.6250 - val_loss: 324939.3438 - val_mse: 324939.3438\n",
      "Epoch 178/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 350775.8125 - mse: 350775.8125 - val_loss: 325865.8125 - val_mse: 325865.8125\n",
      "Epoch 179/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 350631.3125 - mse: 350631.3125 - val_loss: 324513.4062 - val_mse: 324513.4062\n",
      "Epoch 180/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 351294.8438 - mse: 351294.8438 - val_loss: 325119.8438 - val_mse: 325119.8438\n",
      "Epoch 181/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - loss: 350697.1875 - mse: 350697.1875 - val_loss: 324453.8438 - val_mse: 324453.8438\n",
      "Epoch 182/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 350636.5312 - mse: 350636.5312 - val_loss: 327724.7812 - val_mse: 327724.7812\n",
      "Epoch 183/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - loss: 350464.8438 - mse: 350464.8438 - val_loss: 326248.5000 - val_mse: 326248.5000\n",
      "Epoch 184/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step - loss: 350396.2188 - mse: 350396.2188 - val_loss: 325888.0000 - val_mse: 325888.0000\n",
      "Epoch 185/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832us/step - loss: 350722.2500 - mse: 350722.2500 - val_loss: 328551.1250 - val_mse: 328551.1250\n",
      "Epoch 186/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - loss: 351261.3125 - mse: 351261.3125 - val_loss: 326546.9688 - val_mse: 326546.9688\n",
      "Epoch 187/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 350919.6250 - mse: 350919.6250 - val_loss: 325309.2500 - val_mse: 325309.2500\n",
      "Epoch 188/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step - loss: 350903.0625 - mse: 350903.0625 - val_loss: 327329.2500 - val_mse: 327329.2500\n",
      "Epoch 189/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 350739.0938 - mse: 350739.0938 - val_loss: 324337.9375 - val_mse: 324337.9375\n",
      "Epoch 190/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - loss: 350477.9375 - mse: 350477.9375 - val_loss: 325691.1250 - val_mse: 325691.1250\n",
      "Epoch 191/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 350392.7188 - mse: 350392.7188 - val_loss: 326522.1875 - val_mse: 326522.1875\n",
      "Epoch 192/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 834us/step - loss: 350271.9375 - mse: 350271.9375 - val_loss: 324921.9062 - val_mse: 324921.9062\n",
      "Epoch 193/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 349442.3438 - mse: 349442.3438 - val_loss: 326942.4375 - val_mse: 326942.4375\n",
      "Epoch 194/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - loss: 349679.7500 - mse: 349679.7500 - val_loss: 325586.0625 - val_mse: 325586.0625\n",
      "Epoch 195/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - loss: 349525.5312 - mse: 349525.5312 - val_loss: 326391.8125 - val_mse: 326391.8125\n",
      "Epoch 196/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 349285.0625 - mse: 349285.0625 - val_loss: 327720.8438 - val_mse: 327720.8438\n",
      "Epoch 197/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 349826.0625 - mse: 349826.0625 - val_loss: 326769.3750 - val_mse: 326769.3750\n",
      "Epoch 198/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781us/step - loss: 349585.6250 - mse: 349585.6250 - val_loss: 327047.4688 - val_mse: 327047.4688\n",
      "Epoch 199/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 349741.4688 - mse: 349741.4688 - val_loss: 325454.5625 - val_mse: 325454.5625\n",
      "Epoch 200/200\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 349636.4688 - mse: 349636.4688 - val_loss: 327751.4688 - val_mse: 327751.4688\n"
     ]
    }
   ],
   "source": [
    "# Train the model with validation\n",
    "training = model.fit(x_train, y_train, batch_size=64, epochs=200, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[1449.386]\n",
      " [4155.343]\n",
      " [ 542.688]]\n",
      "Actual values: 46519    1781\n",
      "8639     4452\n",
      "23029     631\n",
      "Name: price, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test[:3], verbose=0)\n",
    "print('Predictions:', predictions.round(3))\n",
    "print('Actual values:', y_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498us/step - loss: 357741.7188 - mse: 357741.7188\n",
      "Test loss, accuracy [358109.25, 358109.25]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the testing data\n",
    "results = model.evaluate(x_test, y_test, batch_size=64)\n",
    "print('Test loss, accuracy', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482us/step\n",
      "R-squared score: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "predictions = model.predict(x_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f'R-squared score: {r2:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
